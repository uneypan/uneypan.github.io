---
layout: article
title: 顺序数据
mathjax: true
mermaid: false
---


![](https://drive.google.com/uc?export=view&id=1_NR85DqE2QSpe2_9sBemMXGE1dVDKWGj)

对于许多应用来说，数据集里的数据点独立同分布的假设不成立。顺序数据的数据集通常产⽣于沿着时间序列进⾏的测量，例如某个特定位置的连续若⼲天的降⽔量测量，或者每天汇率的值，或者对于语⾳识别任务，在连续的时间框架下的声学特征。

- 静⽌分布：数据会随着时间发⽣变化，但是⽣成数据的概率分布保持不变。
- ⾮静⽌分布：⽣成概率本⾝会随着时间变化。

我们关注的是**静⽌分布**的情形。

考虑未来的观测对所有之前的观测的⼀个⼀般的依赖关系是不现实的， 因为这样⼀个模型的复杂度会随着观测数量的增加⽽⽆限制地增长。**马尔科夫模型**（Markov model）假定未来的预测仅与最近的观测有关，⽽独⽴于其他所有的观测。

通过引⼊潜在变量，可以得到⼀个更加⼀般的框架，同时仍然保持计算上的可处理性，这就引出了**状态空间模型**（state space model）。

- 潜在变量是离散：**隐马尔可夫模型**（hidden Markov model）。
- 潜在变量服从⾼斯分布：**线性动态系统**（linear dynamical system）。


##  马尔科夫模型


|![](/pictures/prml/图13.2.png)|
|:-:|
|对顺序观测建模的最简单的方法是将它们看做独立的，对应于没有链接的图。|


假设右侧的每个条件概率分布只与最近的一次观测有关，而独立于其他所有之前的观测，那么我们就得到了**一阶马尔科夫链**（first-order Markov chain）

|![](/pictures/prml/图13.3.png)|
|:-:|
|观测 $\{x_n\}$ 的⼀阶马尔科夫链，其中特定的观测$ x_n $的条件概率分布$p(x_n\mid x_{n-1})$依赖于前一次观测$ x_{n−1} $的值。|

这个模型中，$ N $次观测的序列的联合概率分布为

$$ p(x_1,...,x_N) = p(x_1)\prod\limits_{n=2}^Np(x_n\mid x_{n-1}) $$

给定时刻$ n $之前的所有观测，我们看到观测$ x_n $的条件概率分布为

$$ p(x_n\mid x_1,...,x_{n-1}) = p(x_n\mid x_{n-1}) $$

条件概率分布$ p(x_n\mid x_{n−1}) $被限制为相等的，对应于静止时间序列的假设。这样，这个模型被称为**同质马尔科夫链**（homogeneous Markov chain）。

如果我们允许预测除了与当前观测有关以外，还与当前观测的前一次观测有关，那么我们就得到了**二阶马尔科夫链**

|![](/pictures/prml/图13.4.png)|
|:-:|
|观测 $\{x_n\}$ 的二阶马尔科夫链，其中特定的观测$ x_n $依赖于前两次观测$ x_{n−1} $和$ x_{n−2} $的值。|

联合概率分布

$$ p(x_1,...,x_N) = p(x_1)p(x_2\mid x_1)\prod\limits_{n=3}^Np(x_n\mid x_{n-1},x_{n_2}) $$

现在假设我们将模型推广到$ M $阶马尔科夫链，从而联合概率分布由条件概率分布$ p(x_n\mid x_{n−M},...,x_{n-1}) $构建。如果变量是离散变量，且条件概率分布使用一般的条件概率表的形式表示，**那么这种模型中参数的数量为$ K^M (K − 1) $**。 由于这个量**随着$ M $指数增长**，因此通常对于大的$ M $来说，使用这种方法是不实际的。

对于连续变量来说，我们可以使用**线性高斯条件概率分布**，其中每个结点都是一个高斯概率分布，均值是父结点的一个线性函数。这被称为**自回归**（autoregressive）模型或者AR模型。另一种方法是为$ p(x_n\mid x_{n−M},..., x_{n-1}) $使用参数化的模型，例如**神经网络**。这种方法有时被称为抽头延迟线（tapped delay line），因为它对应于存储（延迟）观测变量的前面$ M $个值来预测下一个值。这样，**参数的数量远远小于一个一般的模型**（例如此时参数的数量可能随着$ M $线性增长），虽然这样做会使得条件概率分布被限制在一个特定的类别中。

假设我们希望构造任意阶数的不受马尔科夫假设限制的序列模型，同时能够使用较少数量的自由参数确定。我们可以引入**额外的潜在变量**来使得更丰富的一类模型能够从简单的成分中构建。对于每个观测$ x_n $，我们引入一个对应的潜在变量$ z_n $（类型或维度可能与观测变量不同）。我们现在**假设潜在变量构成了马尔科夫链，得到的图结构被称为状态空间模型**（state space model）。

|![](/pictures/prml/图13.5.png)|
|:-:|
|可以使用潜在变量的马尔科夫链来表示顺序数据，每个观测都以对应的潜在变量的状态为条件。这个重要的图结构组成了隐马尔科夫模型和线性动态系统的基础。|


它**满足下面的关键的条件独立性质**，即给定$ z_n $的条件下，$ z_{n−1} $和$ z_{n+1} $是独立的，从而

$$ z_{n+1} \perp z_{n-1}\mid z_n $$

这个模型的联合概率分布为

$$ p(x_1,...,x_N,z_1,...,z_N) = p(z_1)\left[\prod\limits_{n=2}^Np(z_n\mid z_{n-1})\right]\prod\limits_{n=1}^Np(x_n\mid z_n) $$

使用d-划分准则，我们看到总存在一个路径通过潜在变量连接了任意两个观测变量$ x_n $和$ x_m $，**且这个路径永远不会被阻隔**。因此对于观测变量$ x_{n+1} $来说，给定所有之前的观测，条件概率分布$ p(x_{n+1} \mid x_1,...,x_n) $不会表现出任何的条件独立性，因此我们对$ x_{n+1} $的预测依赖于所有之前的观测。

如果**潜在变量是离散的，那么我们得到了隐马尔科夫模型**（hidden Markov model）。注意，HMM中的观测变量可以是离散的或者是连续的，并且可以使用许多不同的条件概率分布进行建模。**如果潜在变量和观测变量都是高斯变量（结点的条件概率分布对于父结点的依赖是线性高斯的形式），那么我们就得到了线性动态系统**（linear dynamical system）。

## 隐马尔可夫模型

|![](/pictures/prml/图13.6.png)|
|:-:|
|转移图表示一个模型，它的潜在变量有三种可能的状态，对应于三个方框。黑线表示转移矩阵的元素$ A_{jk} $。|
