---
layout: article
title: 贝叶斯
mathjax: True
---


 

> 这个世界上真正的逻辑蕴含在概率的计算之中。——詹姆斯·克拉克·麦克斯韦（1850）


https://en.wikipedia.org/wiki/Vectorization_(mathematics)


Let us introduce the CSP algorithm in the context of EEG signal processing. Consider two classes of EEG signals $X^{(i)} \in \mathbb{R}^{C \times M_i}(i=1,2)$, where $C$ and $M_i$ denote the number of channels and sampled points, respectively. Without loss of generality, hereafter the signal in each channel is assumed to have zero mean. The spatial covariances for the two classes can then be computed as $\hat{R}^{(i)}=\frac{1}{M_i} X^{(i)} X^{(i) T}(i=$ $1,2)$. The task of CSP is to find a linear transform by which the ratio of variance between the two classes can be maximized. Mathematically, this can be formulated as the following optimization problem

<!-- $$
\max _{\mathbf{w}} \quad  \mathbf{w}^T \hat{R}^{(1)} \mathbf{w}\\
\text { s.t. } \mathbf{w}^T \left[\hat{R}^{(1)}+\hat{R}^{(2)}\right] \mathbf{w} = 1
$$ -->

$$
\max _{\mathbf{w}} \frac{\mathbf{w}^T \hat{R}^{(1)} \mathbf{w}}{\mathbf{w}^T \hat{R}^{(2)} \mathbf{w}} \\ \text { s.t. } \quad\|\mathbf{w}\|=1
$$

The solution can be obtained as the eigenvectors of the following generalized eigenvalue decomposition

$$
\hat{R}^{(1)} W=\hat{R}^{(2)} W \Lambda
$$

where $\Lambda$ is a diagonal matrix with eigenvalues. Equivalently, the eigenvectors are given by joint diagonalization of the covariance matrices $\hat{R}^{(1)}$ and $\hat{R}^{(2)}$

$$
W^T \hat{R}^{(i)} W=\Lambda^{(i)}(i=1,2)
$$